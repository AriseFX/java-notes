### 如何使用物理内存?

有且仅有这一种办法：将一个程序加载到内存，PC指向程序首地址， 在CPU取指执行的过程中，内存已经被使用了。 程序是存储在磁盘上的（先忽略加载的过程）， 那么程序该加载到内存的哪个位置呢？

```nasm
(40) _main: mov ax,[100]
             ...
(0)         call 40
```

如果40和0都是真实的物理地址，为了让『call 40』好使，main必须放到物理内存中 的40位置，局限性相当大，要是其他程序也想放到40位置呢，那么先得需要找到空闲内存。

如果恰好1000位置空闲，那么把程序加载到1000位置，并1000赋给IP，CPU开始取值执行， 『call 40』又跳到了物理内存40位置，还是不好使。 仅仅修改PC初始地址是不够的， 还需要另外一个概念：重定位（修改程序中的地址）

![img.png](img/最基本的内存结构.png)

### linux何时重定位?

**编译连接时/加载时？**

    编译时：编译期就得确认内存的位置，对于一些特殊的嵌入式软件，可能是合理的。

    加载时：一旦载入内存就不能动了，必须常驻内存！

**swap**

    高级操作系统都应有swap的概念，在物理内存不够时，将一部分睡眠状态的进程换出
    到磁盘，等到该进程重新换入到内存时，其实又需要重定位！因为这个时候物理内存的
    布局很有可能发生了变化，这就引出了一个概念：运行时重定位。

![img.png](img/img.png)

**运行时重定位**

    每执行一条指令前都需要地址翻译：从逻辑地址算出物理地址。那么采用基地址(base)+偏移量(offset)去定义逻辑地址，
    base就放到PCB中好了，基地址的话x86有对应的基地址寄存器（EB），地址翻译则有相应的硬件MMU，那么每个进程都可
    以拥有自己的内存了。

---

那么整理一下思路，程序载入是整个一起载入内存的吗？

### 分段

    不，我们希望有多个分段：代码/变量/函数库/动态数组/栈，这样可以独立考虑每个段，不会
    写变量的时候不小心把代码给改了，而且每个段扩容也变得很方便。

![img_2.png](img/段.png)

    如何管理不同段的段号/基地址/权限呢？那么就有了LDT表，放到每个进程的PCB中，对应的寄存器为LDTR。

| 段号 | 基址 | 长度 | 保护 |
| :---:| :----: | :----: | :----: |
| 0 | 180k | 150k | R
| 1 | 360k | 60k | R/W

    比如一条指令：jmp 40，那么当前的CS寄存器为40，假如当前代码段的段号为0，
    那么查询段表找到基址加上偏移40，那么就找到了程序跳转的物理地址。

![img_1.png](img/段请求.png)

    仅仅分段还是会有问题。如果现在有多个段需要分配物理内存，那么物理内存该怎么分呢？
    很容易就能想到需要『空闲段表』和『已分配段表』去维护物理内存。如果发起分配请求
    req=100k，发现总空闲内存>100k，但是没有连续的空闲内存>100k，如何分配？这个问题
    就是内存碎片，解决这个问题可以将空闲内存合并（内存缩紧），内存缩紧需要进行段的复制，
    耗费大量的时间！

### 分页

- **从连续到离散**


    那么可以把物理内存分为多个页，分配内存时直接分配任意几个空闲页框，然后使用页表来维护
    内存，页表可以有页号/页框号/特权级。比如现在需要分配n个大小的内存（k个连续的页面），
    并映射到具体的物理内存页框。由于页表的工作方式，现在也不需要那么大的连续内存了！页框
    可以随机分配到物理内存。
    
    例如『mov [0x2240],%eax』
    当前每页大小为4k，算页号需要右移12位，得到页号为0x2偏移为0x240，很快就能找到所在
    物理内存的位置。上述计算过程，为专门的硬件（MMU）完成的。进程切换CR3寄存器跟着切换，
    页表就产生了，MMU基于页表来进行计算。那么页查找的全过程就完成了。

![img_5.jpg](img/img_6.jpg)

- **多级页表和TLB寄存器**

页表该如何设计？

    1.所有的页都有页表：
  
      假如当前是32位系统，可以有4G的寻址空间，页面大小通常为4k，32位地址会有2^20个页面，
    如果那么多个页表项都放在内存中，需要4M内存，如果系统并发10个进程那么就需要40M内存，
    实际上大多数的逻辑地址根本不会用到。//根据程序的『局部性』。

    2.用到的页才有页表项：

      但是这样页号不连续，需要用二分查找，时间复杂度O(log2n)。

既要连续又要降低空间复杂度，那么就有了多级页表。

    那么就抽出一个页目录表作为页表的索引，一来页目录表保存了所有的页目录，可以连续；
    二来被使用到的页目录才会有对应的页表，降低了空间占用。

![img_5.jpg](img/多级页表.jpg)

多级页表引入了新的问题

    每次寻址都需要查询多级页表，增加了访存次数，哪怕碰巧页表项在l1缓存中，性能会下降一到两个
    周期。最差的情况下，每次都会去内存中多取一次数据，性能会下降几十到几百个周期。

    为了降低这一开销，那么就有了TLB。TLB是一组相联快递存储，是一个寄存器，也被称作快表。
    TLB存储了最近使用的页号与其对应页框号，处理流程如下：

      1.CPU产生一个虚拟地址
      
      2.MMU查询TLB，取出对应的页表项（命中）
        
      3.MMU将这个虚拟地址翻译成物理地址，传给内存总线
      
      4.内存总线将物理地址对应的的数据返回给对应的处理器
  
    如果不命中，MMU将会查询高速缓存/内存中的页表，并将新取出的页表项放到TLB中，下图为TLB命中的情况:

![img_5.jpg](img/TLB.jpg)

```cgo
有了TLB后效率呢？基于上述流程可以得到计算公式：
  有效访问时间 = HitR*(TLB+MA)+(1-HitR)*(TLB+2MA)，其中HitR为TLB命中率，
TLB和MA分别为访问耗时。 
    
按命中率98%计算，有效访问时间=98%*(20ns+100ns)+2%*(20ns+200ns)=122ns，近乎访存一次的开销，
上述结果非常依赖TLB的命中率，该命中率得益于『程序』的『局部性』。截取CS:APP书上的一段话：

  ｜尽管在整个运行过程中程序引用的不同页面的总数可能会超过物理内存的总大小，但是『局部性』保证了在
任意时刻，程序将趋于在一个较小的active page集合上工作。在初始开销，也就是将工作集页面调度到内存
中以后，接下来对这个active page的引用将会命中，而不会产生额外的磁盘流量。｜

由此可以得出TLB的命中率非常高。
```

![img_5.png](img/TLB验证.png)

---

### 段页结合 => 虚拟内存

    结合分段和分页，总结出：

      1.应用程序希望能分段(独立考虑每个段)；
      2.物理内存希望能分页(提高分配效率)。

    那么能不能来个东西把他们都组合起来，那么就有了『虚拟内存』，对于用户来虚拟内存是透明，而用户
    程序可以以段的方式去访问虚拟内存。

![img_5.jpg](img/虚拟内存.jpg)

    那么访问内存的方式就变成了『段号+偏移』，用CPU取指执行这一访存动作举例：

        这里逻辑地址为cs:ip，先查段表，找到cs段对应段号的基址，再加上ip组合成虚拟地址，推给MMU，
    MMU根据逻辑地址查询页表，拿到物理地址交给内存总线，获取到对应的指令数据。如下图：

![img_5.jpg](img/地址翻译.jpg)

- **『为什么要有虚拟内存』理论部分就结束了，后面是linux上虚拟内存的源码分析（部分）和虚拟内存的一些拓展应用**

---

### linux内存管理源码分析

思考一个问题，linux何时开始内存管理？ 很容易想到从进程fork中的内存分配开始，

- **换入/换出**

- **其实早已经进入了保护模式**

- **虚拟内存的一些拓展应用**


